{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Goe6hmuos42"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline \n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "matplotlib.rcParams['figure.dpi'] = 144"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "MTnt_S6gpKna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scikit-learn**\n",
        "\n",
        "Scikit-learn, makine öğrenimi için en popüler Python paketidir. Çok sayıda makine öğrenimi modeline sahiptir ve bir makine öğrenimi iş akışı için sıklıkla ihtiyaç duyulan işlevleri sağlar. Göreceğiniz gibi, güzel ve sezgisel bir arayüze sahiptir. Karmaşık makine öğrenimi iş akışları oluşturmayı çok kolaylaştırır. Bu not defteri için Kaliforniya konut verilerini kullanacağız. Veri seti, Kaliforniya'daki her bir sayım bloğu grubu için medyan ev değerini içerir."
      ],
      "metadata": {
        "id": "HI-gM1w1pYdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "#veri al\n",
        "data = fetch_california_housing()\n",
        "X = data['data']\n",
        "y = data['target']\n",
        "\n",
        "print(data['DESCR'])"
      ],
      "metadata": {
        "id": "pPCfTDaxpfP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sınıflar olarak makine öğrenimi modelleri**\n",
        "\n",
        "Scikit-learn büyük ölçüde nesne yönelimli programlama ilkelerine dayanır. Makine öğrenimi algoritmalarını sınıflar olarak uygular ve kullanıcılar bu \"tariflerden\" nesneler oluşturur. Örneğin Ridge, ridge regresyon modelini temsil eden bir sınıftır. Bir Ridge nesnesi oluşturmak için basitçe sınıfın bir örneğini oluştururuz. Python'da sınıf adları CamelCase olarak kullanılır ve her kelimenin ilk harfi büyük yazılır. Scikit-learn bu kuralı benimseyerek sınıfın ne olduğunu ayırt etmeyi kolaylaştırır.\n"
      ],
      "metadata": {
        "id": "vO9aCRZBp6fC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "ridge = Ridge(alpha=0.1)"
      ],
      "metadata": {
        "id": "w23f8mnOp_Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yukarıdaki kodda alpha=0.1 olarak ayarlanmıştır. Burada alpha, ridge modelinin bir hiper parametresidir. Hiperparametreler, öğrenme sürecini yöneten model parametreleridir. Hiyerarşi açısından, normal model parametrelerinin \"üstünde\" yer alırlar. Eğitimden geçtikten sonra model parametrelerinin hangi değerlere eşit olacağını kontrol ederler. Öğrenmeden önce ayarlanan parametreler oldukları için kolayca tanımlanabilirler. Scikit-learn'de hiperparametreler sınıfın bir örneği oluşturulurken ayarlanır. Scikit-learn'in kullandığı varsayılan değerler genellikle iyi bir başlangıç değerleri kümesidir, ancak bu her zaman geçerli değildir. Mevcut hiperparametreleri ve bunların model performansını nasıl etkilediğini anlamak önemlidir.\n",
        "\n",
        "Scikit-learn, makine öğrenimi algoritmalarını tahmin ediciler olarak adlandırır. Üç farklı tahmin edici türü vardır: sınıflandırıcılar, regresörler ve dönüştürücüler. Programatik olarak scikit-learn, tüm tahmin edicilerin miras aldığı BaseEstimator adlı bir temel sınıfa sahiptir. Modeller RegressorMixin, ClassifierMixin ve TransformerMixin olmak üzere ek bir sınıfı miras alır. İkinci sınıfın kalıtımı, modelin ne tür bir tahmin ediciyi temsil ettiğini belirler. Tahmin edicileri arayüzlerine göre iki gruba ayıracağız. Bu iki grup tahmin ediciler ve dönüştürücülerdir."
      ],
      "metadata": {
        "id": "NL0Lo_ZtqGUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tahmin ediciler : Sınıflandırıcılar ve Regresörler\n",
        "\n",
        "Adından da anlaşılacağı gibi, tahmin ediciler tahmin yapan modellerdir. İki ana yöntem vardır.\n",
        "\n",
        "*   fit(X, y): nesneyi X özellik matrisine ve y etiket vektörüne eğitir/uyarlar.\n",
        "\n",
        "*   predict(X): aktarılan X veri seti üzerinde tahminler yapar.\n",
        "\n"
      ],
      "metadata": {
        "id": "euD4GLIVqMfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#model oluşturun ve eğitin/uygun hale getirin.\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "#X üzerindeki etiket değerlerini tahmin eder.\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "print(y_pred)\n",
        "print(\"tahmin dizisinin şekli: {}\".format(y_pred.shape))\n",
        "print(\"eğitim setinin şekli: {}\".format(X.shape))"
      ],
      "metadata": {
        "id": "RT0YyW8iq4ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "predict(X) çıktısının tek boyutlu bir NumPy dizisi olduğunu unutmayın. Dizi, predict yöntemine aktarılan verilerin satır sayısı ile aynı boyuta sahiptir."
      ],
      "metadata": {
        "id": "mo32cv_VrmJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doğrusal regresyon kullandığımız ve verilerimiz sekiz özelliğe sahip olduğu için modelimiz"
      ],
      "metadata": {
        "id": "6p8rqnH6rofs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Katsayılar, uydurulan modelde bir nesnenin özniteliği olarak saklanır. Scikit-learn, uydurma işleminden sonra belirlenen/hesaplanan tüm özniteliklerin alt çizgi ile bittiği bir kuralı benimser. Model katsayıları ve kesişim, sırasıyla coefs_ ve intercept_ öznitelikleri kullanılarak alınır."
      ],
      "metadata": {
        "id": "Dt0WXL9Drql-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"β_0: {}\".format(model.intercept_))\n",
        "\n",
        "for i in range(8):\n",
        "  print(\"β_{}: {}\".format(i+1, model.coef_[i]))"
      ],
      "metadata": {
        "id": "xY8QmGP9sOiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelin bir veri setiyle tahminler yaparken ne kadar iyi performans gösterdiğini bilmek istiyorsak, score(X, y) yöntemini kullanabiliriz. \n",
        "Bu yöntem şu şekilde çalışır\n",
        "\n",
        "1. Tahmin edilen değerleri üretmek için dahili olarak predict(X) çalıştırma.\n",
        "2. Modeli değerlendirmek için tahmin edilen değerleri, yönteme aktarılan gerçek etiket değerleriyle karşılaştırarak kullanma.\n",
        "\n",
        "Değerlendirme denklemi, modelin bir regresör veya sınıflandırıcı olmasına bağlı olarak değişir. Regresyon için, bu(R^2)değeri iken sınıflandırma için doğruluk değeridir."
      ],
      "metadata": {
        "id": "S3ry4yPZsbvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"R^2: {:g}\".format(model.score(X, y)))"
      ],
      "metadata": {
        "id": "Prk31nU3s4tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oldukça basit bir model olan doğrusal regresyon kullandık. Peki ya daha karmaşık bir model kullanmak istersek? Tek yapmamız gereken kolay bir değişiklik; modeller aynı arayüze sahip olduğu için minimum kod yeniden yazımı var. Elbette, farklı modellerin farklı hiperparametreleri vardır, bu nedenle algoritmaları değiştirirken dikkatli olmamız gerekir. Daha karmaşık bir model kullanalım ve onu eğitelim."
      ],
      "metadata": {
        "id": "cjc1MMpAs9FU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "#model oluşturun ve eğitin/uygun hale getirin.\n",
        "model = GradientBoostingRegressor()\n",
        "model.fit(X, y)\n",
        "\n",
        "#X üzerindeki etiket değerlerini tahmin eder\n",
        "y_pred = model_predict(X)\n",
        "\n",
        "print(y_pred)\n",
        "print(\"R^2: {:g}\".format(model.score(X, y)))\n"
      ],
      "metadata": {
        "id": "jX8MJ6ektAhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dönüştürücüler**\n",
        "\n",
        "Dönüştürücüler, bir veri kümesini işleyen ve dönüştüren modellerdir. Bu dönüştürücüler çok kullanışlıdır çünkü verilerimiz nadiren hem eğitim hem de tahmin için doğrudan bir makine öğrenimi modeline beslenecek formdadır. Örneğin, birçok makine öğrenimi modeli, özellikler benzer ölçeklere sahip olduğunda en iyi şekilde çalışır. Tüm dönüştürücüler aynı arayüze sahiptir:,\n",
        "\n",
        "*   fit(X): nesneyi X özellik matrisine eğitir/uyarlar\n",
        "*   transform(X): X üzerinde dönüşümü uygular öğrenilen parametreleri kullanarak\n",
        "*   fit_transform(X): hem fit(X) hem de transform(X) uygular.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6DIntnt0tblk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StandardScaler'ı Kaliforniya konut verileri üzerinde dağıtmanın sonuçlarını özetlemek için pandas kullanacağız"
      ],
      "metadata": {
        "id": "VG1LTwJGuB48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preproccesing import StandardScaler\n",
        "\n",
        "#ölçekleyici oluşturun ve takın\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "\n",
        "#ölçek veri seti\n",
        "Xt = scaler.transform(X)\n",
        "\n",
        "#sonuçları içeren veri çerçevesi oluşturun\n",
        "stats = np.vstack((X.mean(axis=0, X.var(axis=0, Xt.mean(axis=0), Xt.var(axis=0))).T\n",
        "feature_names = data['feature_names']\n",
        "columns = ['unscaled mean', 'unscaled variance', 'scaled mean', 'scaled variance']\n",
        "\n",
        "df = pd.DataFrame(stats, index=feature_names, columns=columns)\n",
        "df"
      ],
      "metadata": {
        "id": "cOjhLWsFuEfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veri çerçevesi, özelliklerimizin nasıl çılgınca farklı ölçeklere sahip olduğunu göstermektedir; ortalama nüfus 1000'in üzerindedir, ancak ortalama oda 5'in biraz üzerindedir. Şimdi, özelliklerimizin her biri sıfır ortalamaya ve bir varyansa sahiptir."
      ],
      "metadata": {
        "id": "lq3ZEwo4vKl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sütun Dönüştürücüleri**\n",
        "\n",
        "Bir makine öğrenimi iş akışı üzerinde çalışırken, verileriniz belirli özellikler için farklı dönüştürme işlemleri gerektirebilir. Ya \"ham\" veri seti sayısal, kategorik ve metin verilerine sahipse. Bu türlerin her biri farklı işleme/dönüştürme işlemleri gerektirir. Bu tür durumları ColumnTransformer adı verilen özel bir dönüştürücü türü kullanarak halledebilirsiniz. Örneğin, Enlem ve Boylam hariç tüm Kaliforniya konut özelliklerinde StandardScalar kullanmak istiyor olabilirsiniz. Bu durumda, ölçeklendirilecek sütunları seçerken diğerlerinin remainder= bağımsız değişkenini kullanarak \"geçmesine\" izin verirsiniz."
      ],
      "metadata": {
        "id": "he8mfh_CvLmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "col_transformer = ColumnTransformer(\n",
        "    remainder='passthrough',\n",
        "    transformes=[\n",
        "        ('scaler', StandardScaler(), slice(0,6) #ilk 6 sütun)\n",
        "    ]\n",
        ")\n",
        "\n",
        "col_transformer.fit(X)\n",
        "Xt = col_transformer.transform(X)\n",
        "\n",
        "print('MedInc dönüşümden önce mi demek istiyorsunuz?', X.mean(axis=0)[0])\n",
        "print('MedInc dönüşümden sonra mı demek istiyorsunuz?', Xt.mean(axis=0)[0], '\\n')\n",
        "\n",
        "print('Boylam dönüşümden önce ne anlama geliyor?', X.mean(axis=0)[-1])\n",
        "print('Dönüşümden sonra boylam ne anlama geliyor?', Xt.mean(axis=0)[-1])\n"
      ],
      "metadata": {
        "id": "tJHfUWBEvVZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sütun dönüştürücüler ayrıca bazı sütunları bırakırken diğerlerinin geçmesine izin vermenizi sağlar. Örneğin, 'MedInc'deki bilgilerin bozulduğunu ve modelimden çıkarılması gerektiğini öğrenirsem, sütun dönüştürücümü 'MedInc'i bırakacak, 'Enlem' ve 'Boylam'ın geçmesine izin verecek ve kalan tüm özellikleri ölçeklendirecek şekilde yeniden yazabilirim."
      ],
      "metadata": {
        "id": "PZPx5DUoxN0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_transformer = ColumnTransformer(\n",
        "    remainder='passthrough',\n",
        "    transformers=[\n",
        "        ('remove', 'drop', 0),\n",
        "        ('scaler', StandardScaler), slice(1,6))\n",
        "    ]\n",
        ")\n",
        "\n",
        "Xt = col_transformer.fit_transform(X)\n",
        "\n",
        "print('Number of features in X:', X.shape[1])\n",
        "print('Number of features Xt:', Xt.shape[1])"
      ],
      "metadata": {
        "id": "G7MAkcL7xOY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pipelines**\n",
        "\n",
        "Analizimiz ve iş akışımız daha karmaşık hale geldikçe, ölçeklendirmeye yardımcı olacak bir araca ihtiyaç duyarız. Örneğin, denetimli bir makine öğrenimi modeli için hazır olmadan önce verilerinize birden fazla dönüşüm uygulamanız gerekebilir. Dönüştürülmüş verilerin ara değişkenlerini oluşturarak dönüşümleri açık bir şekilde uygulayabilirsiniz. Boru hatları, ara dönüşümleri takip etmeyi önlemeye ve daha karmaşık analizler için kodumuzu ölçeklendirmeye yardımcı olan bir yaklaşımdır. Boru hatları Pipeline sınıfı ile yapılır. Esasen bir boru hattı, nihai bir tahmin ediciye sahip bir dizi dönüştürücüyü tutan bir tahmin edici nesnesidir.\n",
        "\n",
        "Bu örnek için şunları yapmak istiyoruz:\n",
        "\n",
        "1. Veri setimizi ölçeklendirin.\n",
        "2. Polinom özellikleri ekleyin.\n",
        "3. Dönüştürülmüş veri setiyle bir doğrusal regresyon modeli eğitin."
      ],
      "metadata": {
        "id": "a0kD8yehjCcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklear.preprocessing import PolynomialFeatures\n",
        "\n",
        "# construct pipeline\n",
        "scaler = StandardScaler()\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "lin_reg = LinearRegression()\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('scaler', scaler),\n",
        "    ('poly', poly_features),\n",
        "    ('regressor', lin_reg)\n",
        "])"
      ],
      "metadata": {
        "id": "ZyO1DV4WjYC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boru hattı, iş akışındaki tüm adımları temsil eden bir tuple listesi geçirilerek oluşturulmuştur. Her bir tuple, adımın adını ifade eden bir dize ve bir tahmin edici nesnesi içerir. Boru hattının adımlarına, adımın adı kullanılarak atıfta bulunulur. name_steps özniteliği, anahtarların adımların adları ve değerlerin adımların tahmin edicileri olduğu bir sözlük döndürür."
      ],
      "metadata": {
        "id": "fDvSabx1kT4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.named_steps"
      ],
      "metadata": {
        "id": "4yRcVoVokYaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boru hattı nesneleri tahmin edicilerdir; aşağıda standart yöntemler çağrıldığında davranışlar listelenmektedir.\n",
        "\n",
        "fit(X, y): tüm dönüştürücülerde sırayla fit_transform(X, y)'yi çağırır ve dönüştürülmüş veri kümesiyle son tahmin ediciyi uyarlar.\n",
        "\n",
        "predict(X): X'i tüm dönüştürücülerle sırayla dönüştürür ve dönüştürülmüş veri setiyle son tahmin ediciyi kullanarak tahmin eder.\n",
        "\n",
        "transform(X): X'i tüm dönüştürücülerle sırayla dönüştürür, yalnızca son tahmin edici None ise çalışır.\n",
        "\n",
        "Yukarıda oluşturulan boru hattı için pipe.fit(X, y) çağrıldığında aşağıdaki işlem gerçekleşir:\n",
        "\n",
        "Xt = scaler.fit_transform(X) \n",
        "Xt = poly.fit_transform(Xt)\n",
        "lin_reg.fit(Xt)\n",
        "\n",
        "pipe.predict(X, y) çağrıldığında, X veri seti dönüştürücülerden geçecek ve son aşamada tahmin ediciyle tahminler yapmak için kullanılacaktır.\n",
        "\n",
        "Xt = scaler.transform(X)\n",
        "Xt = poly.transform(Xt)\n",
        "y_pred = lin_reg.predict(Xt)\n",
        "\n",
        "Tüm iş akışını bir Boru Hattı nesnesi aracılığıyla kapsüllediğimiz için, uydurma, dönüşümler ve tahminler adımlarını manuel olarak çağırmaktan kaçınıyoruz. Hatta kod hacmini daha da azaltmak için tahmin edici nesnelerini boru hattının içinde başlatabiliriz.\n"
      ],
      "metadata": {
        "id": "b127a3jIkaIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# modeli oturtun/eğitin ve etiketleri tahmin edin\n",
        "pipe.fit(X, y)\n",
        "y_pred = pipe.predict(X)\n",
        "\n",
        "print(y_pred)\n",
        "print(\"R^2: {}\".format(pipe.score(X, y)))"
      ],
      "metadata": {
        "id": "jc5QE8A3ksC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Özellik Birliği**\n",
        "\n",
        "FeatureUnion, verilerinizin farklı özellikler için farklı dönüştürme işlemleri gerektirdiği durumlarla başa çıkmak için kullanılan başka bir araçtır. ColumnTransformer gibi, özellikleri ayrı ayrı işler ve sonuçları tek bir özellik matrisinde birleştirir. ColumnTransformer'ın aksine, tüm özellik matrisini nihai bir tahmin ediciye aktarmadan önce farklı dönüştürücüleri ve tahmin edicileri birlikte kullanmanız gereken daha karmaşık iş akışlarını yönetebilir.\n",
        "\n",
        "Pipeline nesneleri tahmin edicileri seri olarak düzenlerken, FeatureUnion nesneleri dönüştürücüleri paralel olarak düzenler. Bir FeatureUnion nesnesi, tek bir çıktı matrisi oluşturmak için her bir dönüştürücünün çıktısını paralel olarak birleştirir. Pipeline ve FeatureUnion nesnelerinin bir kombinasyonunu kullanarak, karmaşık makine öğrenimi iş akışlarını tek bir scikit-learn tahmin edici nesnesi içinde oluşturabiliriz.\n",
        "\n",
        "FeatureUnion'ı göstermek için PCA ve SelectKBest dönüştürücülerini uygulayacağız. PCA, temel bileşen analizi, dönüştürücü orijinal özelliklere dayalı olarak yeni bir ilişkisiz özellikler kümesi döndürürken SelectKBest, geçilen bir kritere dayalı olarak k en iyi özelliği döndürür. Örnek için, seçici etiketlerle en büyük korelasyona sahip 2 özelliği döndürecektir. PCA kullanırken, verilerin sıfır ortalamaya sahip olması gerekir. Sonuç olarak, gerekli iki adımlı süreci temsil eden bir boru hattı nesnesi oluşturuyoruz. PCA nesnesinin 4 ilişkisiz özellik döndürmesini sağlayacağız. PCA ve SelectKBest arasındaki birleşimin sonucu 6 özellikten oluşan bir veri kümesi olacaktır.\n"
      ],
      "metadata": {
        "id": "OuZsnE9plDIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import f_regression, SelectKBest\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "scaler = StandardScaler()\n",
        "pca = PCA(n_components=4)\n",
        "selector = SelectKBest(f_regression, k=2)\n",
        "\n",
        "pca_pipe = Pipeline([('scaler', scaler), ('dim_red', pca)])\n",
        "union = FeatureUnion(['pca_pipe', pca_pipe], ('selector', selector))\n",
        "pipe = Pipeline([('union', union), ('regressor', lin_reg)])\n",
        "pipe.fit(X, y)\n",
        "\n",
        "print(\"orijinal veri setindeki sütun/özellik sayısı: {}\".format(X.shape[-1]))\n",
        "print(\"yeni veri setindeki sütun/özellik sayısı: {}\".format(union.transform(X).shape[-1]))\n",
        "print(\"R^2: {}\".format(pipe.score(X, y)))"
      ],
      "metadata": {
        "id": "cJ5z7w3PlIzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Özel Tahminciler**\n",
        "\n",
        "Scikit-learn çok sayıda makine öğrenimi modeli ve dönüştürücü sağlarken, bazen iş akışımızın gerektirdiği belirli bir modeli veya dönüştürücüyü bize sağlayamayabilir. Ancak kalıtım kavramı sayesinde scikit-learn'in altyapısıyla uyumlu olacak özel bir tahmin edici oluşturabiliriz. Örneğin, özel modelimizin uyum ve tahmin ya da dönüştürme yöntemlerine sahip olmasını istiyoruz. Özel modelimizi Pipeline ve GridSearchCV sınıfı gibi şeylerle kullanmak istiyorsak scikit-learn ile uyumluluk çok önemlidir.\n",
        "\n",
        "Aşağıdaki örnekte, aykırı değerleri, belirli bir aralığın dışındaki değerleri değiştiren özel bir dönüştürücü oluşturacağız. Dönüştürücünün algoritması aşağıdaki gibidir.\n",
        "\n",
        "Her özellik için kabul edilebilir değerlerin alt ve üst sınırını belirleyin. Bu alt ve üst sınırlar \n",
        "inci yüzdelik dilim. Örneğin, bir özellik için %5 ve %95 yüzdelik dilimler sırasıyla 1,3 ve 7,5 ise, (1,3, 7,5) dışındaki tüm değerler aykırı değer olarak kabul edilir.\n",
        "\n",
        "Bir özelliğin her aykırı değeri için, bu değerin yerine \n",
        "özelliğin üçüncü yüzdelik dilimleri. Daha önce olduğu gibi aynı değerleri kullanarak, bir özellik için bir değer 0,7 ise ve kabul edilebilir aralık (1,3, 7,5) ise, 1,3 ile değiştirilir. Değer aralığın \"sağında\", üst sınırdan daha büyük bir yerde bulunuyorsa, 7,5 ile değiştirilir.\n",
        "\n",
        "Bu modeli, gerekli yöntemleri teker teker ekleyerek aşamalı olarak oluşturacağız.\n",
        "\n"
      ],
      "metadata": {
        "id": "JmqcazGjmXuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class OutlierReplacer(BaseEstimator, TransformerMixin):\n",
        "  def__init__(self, q_lower, q_upper):\n",
        "    self.q_lower = q_lower\n",
        "    self.q_upprt = q_upper"
      ],
      "metadata": {
        "id": "ZMbHER7Onuvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bir OutlierReplacer nesnesi oluştururken, alt ve üst sınır için yüzdelik dilimleri belirtmemiz gerekir. Bir sonraki adım, fit yöntemini tanımlamaktır. Fit yönteminin, aykırı değerlerin değiştirilmesi için gereken tüm değerleri hesaplaması gerekir."
      ],
      "metadata": {
        "id": "bjmBSWEtoNDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OutlierReplacer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, q_lower, q_upper):\n",
        "    self.q_lower = q_lower\n",
        "    self.q_upper = q_upper\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    self.upper = np.percentile(X, self.q_upper, axis=0)\n",
        "    self.lower = np.percentile(X, self.q_lower, axis=0)\n",
        "\n",
        "    return self"
      ],
      "metadata": {
        "id": "izyDnMoIoZa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bazı dönüştürücüler etiket değerleriyle çalışır ancak çoğu çalışmaz. Özel dönüştürücümüz etiket değerlerini kullanmadığından, yöntem imzasında y=None olarak ayarladık. Alt ve üst sınırların hesaplanmasının yanı sıra, fit yöntemi self döndürür. Scikit-learn'de, fit yöntemi her zaman self'i, yani uydurulan tahmin edicinin bir kopyasını döndürür. Eğer self döndürmezseniz, özel tahmin ediciniz scikit-learn ile %100 uyumlu olmayacaktır, örneğin Pipeline sınıfı ile çalışmayacaktır. Aşağıda, fit yönteminin dönüştürücünün bir kopyasını döndürdüğünü gösteriyoruz.\n"
      ],
      "metadata": {
        "id": "Mx8mQaDG2Mfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "replacer = OutlierReplacer(5, 95)\n",
        "replacer_copy = replacer.fit(X)\n",
        "\n",
        "print(replacer is replacer_copy)\n",
        "print(id(replacer) == id(replacer_copy))"
      ],
      "metadata": {
        "id": "RcCrvuSZ2Q6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OutlierReplacer(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, q_lower, q_upper):\n",
        "    self.q_lower = q_lower\n",
        "    self.q_upper = q_upper\n",
        "  \n",
        "  def fit(self, x, y=None):\n",
        "    self.upper = np.percentile(X, self.q_upper, axis=0)\n",
        "    self.lower = np.percentil(X, self.q_lower, axis=0)\n",
        "\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    Xt = X.copy()\n",
        "    ind_lower = X < self.lower\n",
        "    ind_upper = X > self.upper\n",
        "\n",
        "    for i in range(X.shape[-1]):\n",
        "      Xt[ind_lower[:, i], i] = self.lower[i]\n",
        "      Xt[ind_upper[:, i], i] = self.upper[i]\n",
        "\n",
        "      return Xt\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "74FAdAJa2g2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dönüştürme yönteminde, aykırı değerler, fit çağrıldığında \"öğrenilen\" değerler olan self.lower ve self.upper içinde saklanan uygun değerlerle değiştirilir. Aktarılan veri setini değiştirmek istemediğimiz için veri setinin bir kopyasını oluşturduğumuzu unutmayın. Python \"pass-by-object-reference\" paradigmasını kullanır; nesne hem fonksiyon hem de çağıran kapsam tarafından paylaşılır. Scikit-learn'de sağlanan dönüştürücülerin varsayılan değeri True olan copy adında bir anahtar kelime argümanına sahip olduğunu unutmayın. Özel dönüştürücümüzün bir nesnesini oluşturalım ve test edelim.\n"
      ],
      "metadata": {
        "id": "ygiYlnbQ3fvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bir dönüştürücü nesnesi oluşturup yerleştirmek ve verileri dönüştürmek\n",
        "replacer = OutlierReplacer(5, 95)\n",
        "replacer.fit(X)\n",
        "Xt = replacer.transform(X)\n",
        "\n",
        "# özellik 0'ın histogramını çizin\n",
        "_, bins, _ = plt.hist(X[:, 0], density=True, bins=40, alpha=0.25, color='b')\n",
        "plt.hist(Xt[:, 0 ], bins=bins, density=True, alpha=0.25, color='r')\n",
        "plt.legend(['original', 'transformed']);"
      ],
      "metadata": {
        "id": "YzVPm-vT3jkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dönüştürülen değerlerin histogramı, uç değerlerin nasıl kesildiğini göstermektedir.\n",
        "\n"
      ],
      "metadata": {
        "id": "1dZq_8br4uI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Özel Regresörler**\n",
        "\n",
        "Bir sonraki örnek için, her zaman eğitim veri setinin ortalama etiket değerini tahmin eden özel bir regresör oluşturacağız. Daha önce olduğu gibi, tahmin ediciyi aşamalı olarak oluşturacağız."
      ],
      "metadata": {
        "id": "5QoGaXNIf5Ql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import RegressorMixin\n",
        "\n",
        "class MeanRegressor(BaseEstimator, RegressorMixin):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.y_mean = np.mean(y)\n",
        "\n",
        "    return self"
      ],
      "metadata": {
        "id": "kH46ratWf9TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit yöntemi, eğitim verilerinin ortalama etiket değerlerini belirler ve bu değeri y_mean içinde saklar. Dönüştürücüde olduğu gibi, fit yöntemi de self değerini döndürür. Eksik olan son metot ise predict metodudur. Bu yöntemde, hesaplanan ortalama etiket değerine başvurmamız ve bu değeri tüm gözlemler için tahmin etmemiz gerekir."
      ],
      "metadata": {
        "id": "CPwsj7m5gVpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MeanRegressor(BaseEstimator, RegressorMixin):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.y_mean = np.mean(y)\n",
        "\n",
        "    return self\n",
        "\n",
        "  def predict(self, X):\n",
        "    return self.y_mean*np.ones(X.shape[0])"
      ],
      "metadata": {
        "id": "YCVzpxb6gYCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MeanRegressor sınıfımız tamamlandığına göre, bunu Kaliforniya konut verileri için kullanabiliriz."
      ],
      "metadata": {
        "id": "ZBJo2-GJg_tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_regressor = MeanRegressor()\n",
        "mena_regressor.fit(X, y)\n",
        "\n",
        "print(mean_regressor.predict(X))\n",
        "print(\"R^2: {}\".format(mean_regressor.score(X, y)))"
      ],
      "metadata": {
        "id": "AWRVQ3-vhAuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tüm tahminlerimiz aynı ve \n",
        " değeri eğitim seti için sıfırdır. Özel bir sınıflandırıcı oluşturma süreci, regressor ile neredeyse aynıdır. Sınıf, RegressorMixin'den miras almak yerine ClassifierMixin'den miras alır."
      ],
      "metadata": {
        "id": "D2BkbDUAhXDh"
      }
    }
  ]
}